# -*- coding: utf-8 -*-
"""WEEK 4 (Data Sci)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VYEq2UcNuIwFqcNNTVup10dBResHRxc8
"""

# Install required libraries
!pip install requests
!pip install selenium
!pip install pandas
!pip install matplotlib seaborn
!apt-get update
!apt install chromium-chromedriver -y

# Import necessary libraries
import requests
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time

# --- Task 1: Web Scraping with API Integration ---
def api_scraping():
    # Example: Fetching GitHub API data for public repositories
    url = "https://api.github.com/repositories"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()  # Extract JSON data
        # Convert JSON data to DataFrame
        df = pd.DataFrame(data, columns=["id", "name", "html_url", "language", "description"])
        # Save to CSV
        df.to_csv("github_repositories.csv", index=False)
        print("GitHub repositories data saved to github_repositories.csv")
        print(df.head())  # Display first few rows
    else:
        print("Failed to fetch data from API:", response.status_code)

# --- Task 2: Handling Dynamic Content with Selenium ---
def selenium_scraping():
    # Configure Chrome for Google Colab
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize WebDriver
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://quotes.toscrape.com/scroll")  # Example with JavaScript-loaded content

    time.sleep(3)  # Allow time for dynamic content to load

    # Extract quotes and authors
    quotes = driver.find_elements(By.CLASS_NAME, "quote")
    data = []
    for quote in quotes:
        text = quote.find_element(By.CLASS_NAME, "text").text
        author = quote.find_element(By.CLASS_NAME, "author").text
        data.append({"quote": text, "author": author})

    # Save to JSON
    with open("quotes.json", "w") as f:
        json.dump(data, f, indent=4)
    print("Quotes data saved to quotes.json")
    driver.quit()

# --- Task 3: Data Cleaning and Analysis ---
def clean_and_analyze():
    # Load data from previous steps (API example)
    df = pd.read_csv("github_repositories.csv")
    print("Initial Data:\n", df.head())

    # Data cleaning
    df = df.drop_duplicates()  # Remove duplicates
    df = df.dropna()  # Remove missing values
    print("Cleaned Data:\n", df.head())

    # Analysis: Top 5 programming languages
    top_languages = df["language"].value_counts().head(5)
    print("Top 5 programming languages:\n", top_languages)

    # Visualization
    sns.barplot(x=top_languages.index, y=top_languages.values)
    plt.title("Top 5 Programming Languages on GitHub")
    plt.xlabel("Language")
    plt.ylabel("Count")
    plt.show()

# --- Task 4: Comparison of Tools and Techniques ---
def compare_tools():
    # Comparison Table
    comparison_data = {
        "Tool": ["BeautifulSoup", "Scrapy", "Selenium"],
        "Speed": ["Fast", "Very Fast", "Moderate"],
        "Ease of Use": ["Easy", "Moderate", "Easy"],
        "Handles Dynamic Content": ["No", "Limited", "Yes"]
    }
    comparison_df = pd.DataFrame(comparison_data)
    print("Comparison of Tools and Techniques:")
    print(comparison_df)

# --- Main Execution ---
if __name__ == "__main__":
    print("Starting Web Scraping Tasks...\n")

    # Task 1: API Integration
    print("--- Task 1: Web Scraping with API Integration ---")
    api_scraping()

    # Task 2: Dynamic Content with Selenium
    print("\n--- Task 2: Handling Dynamic Content with Selenium ---")
    selenium_scraping()

    # Task 3: Data Cleaning and Analysis
    print("\n--- Task 3: Data Cleaning and Analysis ---")
    clean_and_analyze()

    # Task 4: Tool Comparison
    print("\n--- Task 4: Comparison of Tools and Techniques ---")
    compare_tools()